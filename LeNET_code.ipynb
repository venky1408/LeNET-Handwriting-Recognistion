{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS-j0MjnDwY9"
   },
   "source": [
    "Import Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIPqNhQ8DnYu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdVFqMWRD33b"
   },
   "source": [
    "Load and Transform the MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMIkK5lrDzgy"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the dataset\n",
    "])\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Data loaders for the training and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePM_l1bED7zI"
   },
   "source": [
    "Define the LeNet-Like Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qd7Aj-bTD9lF"
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)  # Convolutional layer with 20 output channels\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)  # Convolutional layer with 50 output channels\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(500, 10)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0DKFvr_EAQb"
   },
   "source": [
    "Initialize the Network and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfLFaFk4ECTH"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LeNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksTZthYrEFy7"
   },
   "source": [
    "Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BblVuAwEGiC",
    "outputId": "fcd574a8-331f-42a6-da57-fa185f9a8237"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "### Step 6: Evaluate the Model\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "\n",
    "### Step 7: Run the Training and Testing\n",
    "for epoch in range(1, 11):  # Run for 10 epochs\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6ffwSP5EV1r"
   },
   "source": [
    "Visualizing the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqM1HzAs_yFF",
    "outputId": "1ce466ea-72f1-4b92-f93b-d1b2afcc6938"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the model and generate confusion matrix\n",
    "def evaluate_confusion_matrix(model, device, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy().flatten())\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return cm\n",
    "\n",
    "# Call the function to evaluate confusion matrix\n",
    "conf_matrix = evaluate_confusion_matrix(model, device, test_loader)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "KdsoP9ux_3BX",
    "outputId": "823bff95-3ba5-4271-fce9-d2acd28c476c"
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Evaluate the model on the test set and collect predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        true_labels.extend(target.cpu().numpy())\n",
    "        predicted_labels.extend(pred.cpu().numpy().flatten())\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJnss54xEp0n"
   },
   "source": [
    "A complete implementation for training and evaluating the LeNet-5 network on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "mK20B8H9_-7e",
    "outputId": "a78ecfb0-9dca-4444-fb14-8191a074c340"
   },
   "outputs": [],
   "source": [
    "def plot_model_comparison():\n",
    "    original_lenet = {\n",
    "        \"Layer\": [\"Conv1\", \"Max Pooling\", \"Conv2\", \"Max Pooling\", \"Fully Connected\"],\n",
    "        \"Output Channels/Neurons\": [20, \"-\", 50, \"-\", 120]\n",
    "    }\n",
    "\n",
    "    modified_lenet = {\n",
    "        \"Layer\": [\"Conv1\", \"Max Pooling\", \"Conv2\", \"Max Pooling\", \"Fully Connected\"],\n",
    "        \"Output Channels/Neurons\": [30, \"-\", 60, \"-\", 500]\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Hide axes\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Create table\n",
    "    table_data = [[\"Layer\", \"Original LeNet\", \"Modified LeNet\"]]\n",
    "    for layer, original_output, modified_output in zip(original_lenet[\"Layer\"], original_lenet[\"Output Channels/Neurons\"], modified_lenet[\"Output Channels/Neurons\"]):\n",
    "        table_data.append([layer, str(original_output), str(modified_output)])\n",
    "\n",
    "    ax.table(cellText=table_data[1:], colLabels=table_data[0], loc='center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the comparison\n",
    "plot_model_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6IbyhP0F68u"
   },
   "source": [
    "Visualizing the training progress of the LeNet-5 network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RuyVR2nuB_Id",
    "outputId": "14164b68-972a-4cc1-e25e-a2795716d65d"
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Progress')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training Accuracy Progress')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, train_losses, train_accuracies):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total_samples += len(data)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "    train_accuracy = 100. * correct / total_samples\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "# Initialize empty lists to store training progress\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Run the Training Loop\n",
    "for epoch in range(1, 11):  # Run for 10 epochs\n",
    "    train(model, device, train_loader, optimizer, epoch, train_losses, train_accuracies)\n",
    "\n",
    "# Plot the training progress\n",
    "plot_training_progress(train_losses, train_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4oyy6OBGSUC"
   },
   "source": [
    "Comparision of Model Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "jdFc3oePEIT_",
    "outputId": "a6e9534b-c46b-4f3d-e624-74d47fe35f9c"
   },
   "outputs": [],
   "source": [
    "# Define data\n",
    "epochs = range(1, 11)\n",
    "modified_lenet_accuracies = [94.95, 96.19, 96.71, 96.98, 97.19, 97.36, 97.51, 97.66, 97.78, 97.89]\n",
    "original_lenet_accuracies = [94.56, 95.86, 96.35, 96.61, 96.83, 96.99, 97.14, 97.29, 97.42, 97.53]\n",
    "simple_mlp_accuracies = [91.75, 92.84, 93.47, 93.82, 94.10, 94.35, 94.58, 94.79, 94.97, 95.14]\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, modified_lenet_accuracies, label=\"Modified LeNet\")\n",
    "plt.plot(epochs, original_lenet_accuracies, label=\"Original LeNet\")\n",
    "plt.plot(epochs, simple_mlp_accuracies, label=\"Simple MLP\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Comparison of Model Accuracies\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
